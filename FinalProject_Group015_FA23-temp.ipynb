{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Professor Recommendations and the Relationship between Expected and Received Grades\n",
    "\n",
    "Video Link:\n",
    "\n",
    "# Permissions\n",
    "\n",
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that student names will be included (but PIDs will be scraped from any groups who include their PIDs).\n",
    "\n",
    "* [ X ] YES - make available\n",
    "* [  ] NO - keep private\n",
    "\n",
    "# Names\n",
    "\n",
    "- Pallavi Prabhu\n",
    "- Hana Ton-Nu\n",
    "- Justin Gamm\n",
    "- Raquel Sanchez\n",
    "- Ria Singh\n",
    "\n",
    "# Abstract\n",
    "\n",
    "Our team was interested in learning whether or not a student's opinion on a professor would result in a higher expectation of their class grade. We decided to look into our own school's evaluation system, CAPEs (Course and Professor Evaluations) in order to answer this question. The data was relatively clean, however, we wished to only include necessary information. In order to do this we dropped all rows with NaN values, created a function to convert necessary information into float objects, and created a new column that would help with our analysis later. We also decided to drop the entire Instructor row out of respect and privacy for the professors, we also deemed it unnecessary for our final analysis.\n",
    "\n",
    "We then began our Data Analysis and produced some interesting results. We found that most students were fairly accurate in estimating their final grades. We also found that most evaluations, whether it be for the class or professor, are fairly positive. We decided to look at a small subset in order to see if there was any correlation between expected grades and professor recommendation. From that we weren't able to get any viable information. We also noticed a large positive jump in final grades for the year 2020, likely due to COVID and more lax class setups. \n",
    "\n",
    "After running all of our analyses we saw that there was a slight increase in grades depending on more positive reviews of a professor, however it is not a large increase. We also noted that just being in a stem class will have a larger effect on a student's expected and received grade than their recommendation of the professor. \n",
    "\n",
    "We concluded that our hypothesis was incorrect, but not in the way we expected. We found that the more positive ratings a professor had, the higher average _received_ grade. This does not coincide with our hypothesis, which stated the expected grade would be higher. However, we do have evidence for a higher grade expected _and_ received from professors with positive ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do students at UCSD expect higher grades than they receive if they recommend the professor, as measured through CAPEs? Namely, do students that recommend a professor generally expect to do better (expected grade) than they actually did (grade received) and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAPES stands for “Course And Professor Evaluations” and are offered to students at the end of every quarter to give feedback on their courses and the professors that taught them. As stated on the website, CAPES<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) is: \n",
    "\n",
    "    “a student-run organization that administers a standardized evaluation of UCSD's undergraduate courses and professors. Student feedback gauges the caliber of both the University's curriculum and its faculty. We provide students with the opinions of their peers on any particular course or professor.” (UCSD)\n",
    "    \n",
    "After students submit their CAPES, this data is aggregated for each quarter of evaluations into features like the percentage of students that recommended the class, the percentage of students that recommend the instructor, the number of hours per week involved in this class, the average grade expected, and the average grade received (which is calculated from the final submitted grades). CAPEs play an important role in helping students choose their professors and even the courses they enroll them in as they generally have more statistics and comprehensive data compared to other rating sites. \n",
    "\n",
    "In addition, other rating sites often show signs of significant skew, with a majority of ratings being either towards the extreme since people are more likely to rate something they have a strong reaction towards. This creates a bimodal distribution that lacks data in between these two strong opinions. On the other hand, while not entirely protected, CAPEs likely have some resistance to potential skews in data since students are often incentivized to complete them for extra credit, which gives equal opportunity for various kinds of responses.\n",
    "In winter of 2021, a COGS108 group also asked a question regarding CAPEs: “How has remote learning during the COVID-19 pandemic affected UCSD students’ grades and learning experience?” (Group084_Wi21<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2)). Thus, while the general topic remains the same, while they focus on the change of grades and learning as a result of the pandemic, we want to explore the potential association between students’ like of professors (recommended professor) and their potential overestimation/underestimation of their performance in class. While it seems obvious that students who do better in a course likely recommend a professor at a higher rate than those who don’t, we are more interested in their confidence and expectation.\n",
    "\n",
    "Furthermore, prior studies that have explored this topic of student-professor evaluations have found evidence of gender bias where female professors tended to rank lower than their male counterparts<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3). While this is not directly related to our question, the general idea remains the same and it peaked our curiousity on some of the other featurs we could include. This could also be an interesting feature to consider in our analysis: how the professors’ gender, likeability, and the students' expectation of grade vs their actual received grade are all related. Do students overestimate their ability and expect a higher grade when the professor is female? How does the gender of a professor play into their recommendation in relation to the grades received in the class? Some other features that pique interest are the difference in expectation vs received depending on the department or type of class (like STEM vs liberal arts). Based on experience and expectatioon, STEM classes are notoriously perceived as more difficult than liberal arts class. Thus, it may be interesting to explore how this perceived difficulty plays into the relationship between the difference in grades expected and received and the overall likeabilty/recommendation of a professor. \n",
    "\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) University of California, San Diego. (n.d.). Course And Professor Evaluations. COURSE AND PROFESSOR EVALUATIONS (CAPE). Retrieved November 1, 2023, from https://cape.ucsd.edu/\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) FinalProject_group084. (2021, March 29). GitHub. Retrieved November 1, 2023, from https://github.com/COGS108/FinalProjects-Wi21/blob/main/FinalProject_group084.ipynb\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3) Mitchell, K., & Martin, J. (2018). Gender Bias in Student Evaluations. PS: Political Science & Politics, 51(3), 648-652. doi:10.1017/S104909651800001X from https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/gender-bias-in-student-evaluations/1224BE475C0AE75A2C2D8553210C4E27\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After thinking back on our experiences in classes at UCSD and talking to our peers, we hypothesize that students in classes of well liked professors will overestimate their grade and those in classes of professors who are not as well liked will underestimate their grade.\n",
    "\n",
    "We expect the correlation between the recommendation percentage of a professor and the difference between the expected and received grades to be positive. Additionally we expect the difference to become negative in professors who have low recommendation percentages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "- Dataset #1\n",
    "  - Dataset Name: CAPES data from UCSD\n",
    "  - Link to the dataset: https://www.kaggle.com/datasets/sanbornpnguyen/ucsdcapes/\n",
    "  - Number of observations: 63363\n",
    "  - Number of variables: 11\n",
    "\n",
    "\n",
    "This dataset includes CAPES data from UCSD. Important variables in this dataset that drive our research include recommendation percent, expected grade, and received grade. The datatypes included in this dataset include object, int, and float. The metric in our case would be the difference between received grade and expectated grade, which we created a column to display. These variables can act as a proxy for teaching effectiveness. In order to clean up our data, we used a couple different methods; we dropped unecessary columns, we standardized the grades and percentage columns, and dropped na columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data Process\n",
    "  - Dataset is relatively clean except for 'Average Grade Recieved' column containing some NaN. We determined that the data was in a usable format because each measured variable had its own column, each observation of the measured variable was in a different row, and the column headers are descriptive of their variables.\n",
    "  - We first decided to drop the evaluation url column since we did not deem it as relevant information needed to answer our question.\n",
    "  - We then checked and dropped all rows with null information since we felt those with missing values would not be helpful and would make exctracting data more difficult.\n",
    "  - We created a standarisation function that would convert all of the values in the 'Average Grade Expected' and 'Average Grade Received' columns to float columns and remove the '%' symbol.\n",
    "  - A new column was created called 'Difference' where the difference between the 'Average Grade Received' and the 'Average Grade Expected' is stored. This will help later to determine whether students over or underestimated their grades.\n",
    "  - We created columns for both the year and quarters so it has integers representing the quarters (1 = Winter, 2 = Spring, 3 = S1, 4 = S2, 5 = S3, 6 = Fall, 7 = SU) and the full year is displayed.\n",
    "  - Lastly, we also the categories of 'liberal' and 'stem' for the top 100 class codes for which information could be found in order to use these two groups in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes = pd.read_csv(\"capes_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine number of variables and observations\n",
    "capes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping column as we cannot use it\n",
    "capes = capes.drop(columns = [\"Evalulation URL\", \"Instructor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for NA \n",
    "capes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigating if NA in grades expected and recieved are from particular quarter(covid?)\n",
    "capes_na = capes[capes.isnull().any(axis=1)]\n",
    "capes_na.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes_na.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes_na[\"Quarter\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all rows with NA as remaining dataset is still large enough and the NA rows are random\n",
    "capes = capes.dropna()\n",
    "capes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make the Average Grade Expected and Average Grade Recieved into floats of just the grade point\n",
    "#number\n",
    "def grade_standardize(grade):\n",
    "    \n",
    "    #ensure grade has not been standardized\n",
    "    if type(grade) == float:\n",
    "        return grade\n",
    "    \n",
    "    #retain only part after open parenthesis\n",
    "    grade = grade.split(\"(\")[-1]\n",
    "    \n",
    "    #remove close parenthesis\n",
    "    grade = grade.replace(\")\", \"\")\n",
    "    \n",
    "    #change to float\n",
    "    grade = float(grade)\n",
    "    \n",
    "    return grade\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes[\"Average Grade Expected\"] = capes[\"Average Grade Expected\"].apply(grade_standardize)\n",
    "capes[\"Average Grade Received\"] = capes[\"Average Grade Received\"].apply(grade_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove % symbol from Percentage Recommended Class and Percentage Recommended Professor and make\n",
    "#into float\n",
    "\n",
    "def percent_standardize(percent):\n",
    "    #ensure percent has not been standardized\n",
    "    if type(percent) == float:\n",
    "        return percent\n",
    "    \n",
    "    #remove % sign\n",
    "    percent = percent.replace(\"%\", \"\")\n",
    "    \n",
    "    #convert to float\n",
    "    percent = float(percent)\n",
    "    \n",
    "    return percent\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes[\"Percentage Recommended Class\"] = capes[\"Percentage Recommended Class\"].apply(percent_standardize)\n",
    "capes[\"Percentage Recommended Professor\"] = capes[\"Percentage Recommended Professor\"].apply(percent_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column called difference which is the difference between the average\n",
    "#grade expected and the average grade recieved\n",
    "#negative numbers in difference signify students overestimated their grade, positive number signifies students\n",
    "#underestimated their grade\n",
    "capes = capes.assign(Difference = capes[\"Average Grade Received\"] - capes[\"Average Grade Expected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns for year and terms where each year will be an int (eg. 07, 08, 23, etc) and terms will be\n",
    "#denoted by numbers(see code)\n",
    "#this code is taken from https://github.com/COGS108/FinalProjects-Wi21/blob/main/FinalProject_group084.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_term(term):\n",
    "    TERMS = { \"WI\": 1, \"SP\": 2, \"S1\": 3, \"S2\": 4, \"S3\": 5, \"FA\": 6, \"SU\" : 7}\n",
    "    \n",
    "    term = term[:2]\n",
    "    term = TERMS[term]\n",
    "    \n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(qtr):\n",
    "    \n",
    "    year = qtr[2:]\n",
    "    year = int(\"20\" + year)\n",
    "    \n",
    "    return year\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes[\"Term\"] = capes[\"Quarter\"].apply(extract_term)\n",
    "capes[\"Year\"] = capes[\"Quarter\"].apply(extract_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capes.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the types of courses in the dataset for potential grouping based on STEM vs liberal\n",
    "#using value counts soted by descending to get the top 100 codes\n",
    "capes['Course'].apply(lambda x: x.split(' ')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories given for the top 100 course codes\n",
    "#note some classes can be considered both since they are multidiciplinary like COGS\n",
    "#classes offered as both BS and CA: COGS, ESYS, HDS, SYN, ICAM\n",
    "#unsure due to no information found COSF, COGN, CONT, SOCC\n",
    "stem_courses = ['MATH', 'CSE', 'CHEM', 'PHYS', 'MAE', 'ECE', 'BILD', 'SE', 'BENG', 'BIPN', 'BIEB', 'NANO'\\\n",
    "               'DSC','ENG', 'BISP', 'FPMU' ]\n",
    "liberal_courses = ['ECON','POLI', 'MGT', 'PSYC', 'VIS', 'SOCI', \\\n",
    "                   'COGS', 'COMM', 'MUS', 'PHIL', 'SIO', 'TMDV', 'JAPN', 'CHIN', 'EDS', 'MMW', 'ETHN', 'USP', \\\n",
    "                   'LIGN', 'TDGE', 'ANTH', 'HIUS', 'AWP', 'LTWL', 'DOC', 'FMPH', 'HILD', 'HDP', 'INTL', \\\n",
    "                   'LSTP', 'HIEA', 'GLBH', 'CGS', 'WCWP', 'TDHT', 'HILA', 'ESYS', 'ANBI', 'HINE', 'COCU', \\\n",
    "                   'LTKO', 'LIHL', 'RELI', 'COHI', 'ENVR', 'LTCS', 'HDS', 'ANAR', 'LTLA', 'TDTR', 'HITO'\\\n",
    "                   'LATI', 'TDDR', 'HIAF', 'LTEU', 'TWS', 'DSGN', 'LTRU', 'TDDE', 'ELWR', 'SOCB', 'LAWS'\\\n",
    "                   'LTCH', 'LTIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to add this label based on the class code\n",
    "def course_cat(code):\n",
    "    if code in stem_courses:\n",
    "        return 'stem'\n",
    "    elif code in liberal_courses:\n",
    "        return 'liberal'\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Carry out whatever EDA you need to for your project.  Because every project will be different we can't really give you much of a template at this point. But please make sure you describe the what and why in text here as well as providing interpretation of results and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Analysis You Did - Give it a better title\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Analysis You Did - Give it a better title\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETC AD NASEUM\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thoughtful discussion of ethical concerns included\n",
    "- Ethical concerns consider the whole data science process (question asked, data collected, data being used, the bias in data, analysis, post-analysis, etc.)\n",
    "- How your group handled bias/ethical concerns clearly described\n",
    "\n",
    "Acknowledge and address any ethics & privacy related issues of your question(s), proposed dataset(s), and/or analyses. Use the information provided in lecture to guide your group discussion and thinking. If you need further guidance, check out [Deon's Ethics Checklist](http://deon.drivendata.org/#data-science-ethics-checklist). In particular:\n",
    "\n",
    "- Are there any biases/privacy/terms of use issues with the data you propsed?\n",
    "- Are there potential biases in your dataset(s), in terms of who it composes, and how it was collected, that may be problematic in terms of it allowing for equitable analysis? (For example, does your data exclude particular populations, or is it likely to reflect particular human biases in a way that could be a problem?)\n",
    "- How will you set out to detect these specific biases before, during, and after/when communicating your analysis?\n",
    "- Are there any other issues related to your topic area, data, and/or analyses that are potentially problematic in terms of data privacy and equitable impact?\n",
    "- How will you handle issues you identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discusison and Conclusion\n",
    "\n",
    "Wrap it all up here.  Somewhere between 3 and 10 paragraphs roughly.  A good time to refer back to your Background section and review how this work extended the previous stuff. \n",
    "\n",
    "\n",
    "# Team Contributions\n",
    "\n",
    "Speficy who did what.  This should be pretty granular, perhaps bullet points, no more than a few sentences per person."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
